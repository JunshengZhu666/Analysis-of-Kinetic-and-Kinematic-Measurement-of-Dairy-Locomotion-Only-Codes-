{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8405cd93",
   "metadata": {},
   "source": [
    "### Load in kinetic data as: X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f15d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of examples:  70\n",
      "dimension of one examples:  70 50\n",
      "obtain array X with shape:  (70, 70, 50)\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "############## read in kinetic data ##################################\n",
    "#####################################################################\n",
    "\n",
    "## read in kinetic data \n",
    "## into shape \n",
    "## x_train:  (data size, rows, columns)\n",
    "## x_test:  (data size, rows, columns)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import glob\n",
    "#import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# list of names of files \n",
    "file_names = []\n",
    "# X is the data before preprocess\n",
    "X = np.zeros((70, 70, 50))\n",
    "# load in every excel file from kinetic_processed\n",
    "for i, xls_file in enumerate(glob.glob(\"kinetic_processed/*\")):\n",
    "    # append into file_names list\n",
    "    file_names.append(xls_file)\n",
    "    #print(xls_file)\n",
    "    # exclude column index and create panda dataframe\n",
    "    dataframe = pd.read_excel(xls_file).iloc[:, 1:]\n",
    "    # convert into numpy array and store into X\n",
    "    matrix = dataframe.to_numpy()\n",
    "    #print(matrix.shape)\n",
    "    X[i] = matrix\n",
    "    \n",
    "print(\"total number of examples: \", X.shape[0])\n",
    "print(\"dimension of one examples: \", X.shape[1], X.shape[2])\n",
    "print(\"obtain array X with shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48844a",
   "metadata": {},
   "source": [
    "### Cut one matrix into four hoofs as: hoof_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8dbb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "############## cut one matrix into four hoofs as: hoof_set  #########\n",
    "#####################################################################\n",
    "\n",
    "# init a new dataset to store hoofs: hoof_set \n",
    "#hoof_set = np.zeros((280, 30, 20))\n",
    "hoof_set = []\n",
    "# loop through all 70 cows \n",
    "for cow in range(70):\n",
    "    # take one matrix \n",
    "    one_matrix = X[cow, :, :]\n",
    "\n",
    "    # for each hoof, we crop it with height 30 and width 20 \n",
    "    # top left hoof\n",
    "    top_left = one_matrix[:30, :20] \n",
    "    # top right hoof \n",
    "    top_right = one_matrix[:30, -20:] \n",
    "    # bottom left hoof \n",
    "    bottom_left = one_matrix[-30:, :20] \n",
    "    # bottom right hoof \n",
    "    bottom_right = one_matrix[-30:, -20:] \n",
    "\n",
    "    #imshow(top_left)\n",
    "    #imshow(top_right)\n",
    "    #imshow(bottom_left)\n",
    "    #imshow(bottom_right)\n",
    "    hoof_set.append(top_left)\n",
    "    hoof_set.append(top_right)\n",
    "    hoof_set.append(bottom_left)\n",
    "    hoof_set.append(bottom_right)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc2876",
   "metadata": {},
   "source": [
    "### Inspect each 4 hoofs in hoof_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d5f4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'use function to check each 70 hoof sets (4 hoofs in each set)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "############## inspect each 4 hoofs in hoof_set  ####################\n",
    "#####################################################################\n",
    "\n",
    "\"\"\"\n",
    "args\n",
    "    -- check: index of cow that need to check \n",
    "    from 0-70\n",
    "return \n",
    "    -- print four hoofs of that cow\n",
    "\"\"\"\n",
    "def check_4hoofs(check): \n",
    "    \n",
    "    # base line \n",
    "    if check > 70:\n",
    "        return \"invalid number\"\n",
    "    \n",
    "    check = check * 4\n",
    "    \"\"\"    imshow(hoof_set[check])\n",
    "        imshow(hoof_set[check+1])\n",
    "        imshow(hoof_set[check+2])\n",
    "        imshow(hoof_set[check+3])\"\"\"\n",
    "    \n",
    "    # Here we create a figure instance, and two subplots\n",
    "    fig = plt.figure(figsize = (5,20)) # width x height\n",
    "    ax1 = fig.add_subplot(4, 1, 1) # row, column, position\n",
    "    ax2 = fig.add_subplot(4, 1, 2)\n",
    "    ax3 = fig.add_subplot(4, 1, 3)\n",
    "    ax4 = fig.add_subplot(4, 1, 4)\n",
    "\n",
    "\n",
    "    # We use ax parameter to tell seaborn which subplot to use for this plot\n",
    "    ax1.imshow(hoof_set[check],  interpolation='nearest')\n",
    "    ax2.imshow(hoof_set[check+1],  interpolation='nearest')\n",
    "    ax3.imshow(hoof_set[check+2],  interpolation='nearest')\n",
    "    ax4.imshow(hoof_set[check+3],  interpolation='nearest')\n",
    "\n",
    "#print(\"inspect hoof_set: \", len(hoof_set))\n",
    "\"\"\"use function to check each 70 hoof sets (4 hoofs in each set)\"\"\"\n",
    "#check_4hoofs(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faff7e3",
   "metadata": {},
   "source": [
    "### Load in lameness NRS score as: Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69f9814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (66, 70, 50)\n",
      "Y.shape:  (66,)\n"
     ]
    }
   ],
   "source": [
    "## load in excel file\n",
    "# name of the label file \n",
    "excel_name = 'labels.xlsx'\n",
    "# read in by panda\n",
    "df = pd.read_excel(excel_name)\n",
    "# only need the second column, where we store the score\n",
    "Y = df.iloc[:, 2]\n",
    "\n",
    "\n",
    "## clean some missing data where we labeled -1  and display the label distribution \n",
    "# exclude the missing data, where I put -1 for NRS, \n",
    "X_clean = []\n",
    "Y_clean = []\n",
    "# if labeled as -1, not append that example/label\n",
    "for i, j in enumerate(Y): \n",
    "    if j == -1: \n",
    "        pass \n",
    "    else: \n",
    "        X_clean.append(X[i])\n",
    "        Y_clean.append(Y[i])\n",
    "# store as numpy array and inspect the shape as X and Y\n",
    "X = np.array(X_clean)\n",
    "Y = np.array(Y_clean)\n",
    "print(\"X.shape: \", X.shape) \n",
    "print(\"Y.shape: \", Y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1232d9",
   "metadata": {},
   "source": [
    "### Plot and inspect Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03477e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# func to print the number for different scores \n",
    "from collections import Counter\n",
    "\"\"\"\n",
    "arg: \n",
    "    -- y: a list \n",
    "return: \n",
    "    -- a histogram\n",
    "\"\"\"\n",
    "def inspect_y(y, plot=True): \n",
    "    print(\"----- Set of value in the list -----\")\n",
    "    print([i for i in Counter(y).keys()]) # equals to list(set(words))\n",
    "    print(\"----- Count of value in the list -----\")\n",
    "    print([i for i in Counter(y).values()]) # counts the elements' frequency\n",
    "    \n",
    "    if plot:\n",
    "        data = y\n",
    "        plt.xlim([min(data)-1, max(data)+1])\n",
    "        plt.hist(data, alpha=0.5)\n",
    "        plt.title('Distribution of Y label')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('counts')\n",
    "        plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f344d",
   "metadata": {},
   "source": [
    "### In the label, the missed NRS are labeled with -1, we have to delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c28357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of Y:  70\n",
      "any null data:  False\n",
      "missed_idx:  4 [58, 62, 63, 69]\n",
      "re_ls:  16 [232, 233, 234, 235, 248, 249, 250, 251, 252, 253, 254, 255, 276, 277, 278, 279]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"reload the data to find the index of label -1\"\"\"\n",
    "\n",
    "# name of the label file \n",
    "excel_name = 'labels.xlsx'\n",
    "# read in by panda\n",
    "df = pd.read_excel(excel_name)\n",
    "# only need the second column, where we store the score\n",
    "temp_Y = df.iloc[:, 2]\n",
    "\n",
    "print(\"the length of Y: \", len(temp_Y))\n",
    "print(\"any null data: \", (temp_Y==None).any())\n",
    "\n",
    "# loop through temp_Y to find the index \n",
    "# init list to store index \n",
    "missed_idx = []\n",
    "for i, j in enumerate(temp_Y): \n",
    "    if j == -1: \n",
    "        missed_idx.append(i)\n",
    "        \n",
    "# inspect the list \n",
    "print(\"missed_idx: \", len(missed_idx), missed_idx)\n",
    "\n",
    "\"\"\"now we know that the index that we need to discard in ls_hoof_int \n",
    "58 * 4 (and the next 3)\n",
    "62 * 4 (and the next 3)\n",
    "63 * 4 (and the next 3)\n",
    "69 * 4 (and the next 3)\n",
    "\"\"\"\n",
    "\n",
    "# function to create this list from missed_idx\n",
    "def time4add3num(ls): \n",
    "    # init return ls \n",
    "    re_ls = []\n",
    "    for i in ls: \n",
    "        time4 = i*4\n",
    "        re_ls.append(time4)\n",
    "        re_ls.append(time4+1)\n",
    "        re_ls.append(time4+2)\n",
    "        re_ls.append(time4+3)\n",
    "    return re_ls \n",
    "\n",
    "# inspect this list \n",
    "re_ls = time4add3num(missed_idx)\n",
    "print(\"re_ls: \", len(re_ls), re_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b656d8f",
   "metadata": {},
   "source": [
    "# Try to extract variable from the hoof and correlate to NRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b420aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: \n",
      "re_ls:  16\n",
      "hoof_set_66:  (264, 30, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input data: \")\n",
    "#print(\"hoof_set: \", hoof_set.shape)\n",
    "print(\"re_ls: \", len(re_ls))\n",
    "\n",
    "# firstly, discard 16 hoofs that have no NRS \n",
    "def discard_no_NRS(hoof_set, re_ls): \n",
    "    # init a list to store result \n",
    "    result = [] \n",
    "    # loop thourgh hoof_set\n",
    "    for i in range(hoof_set.shape[0]): \n",
    "        # if is in the list of non-scored, discard\n",
    "        if i in re_ls: \n",
    "            pass\n",
    "        # if not in the re_ls, append\n",
    "        else: \n",
    "            result.append(hoof_set[i])\n",
    "            \n",
    "    # return result as array \n",
    "    return np.array(result)\n",
    "\n",
    "# use this function and inspect shape \n",
    "hoof_set_66 = discard_no_NRS(np.array(hoof_set), re_ls)\n",
    "print(\"hoof_set_66: \", hoof_set_66.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef54400",
   "metadata": {},
   "source": [
    "### Now, extract five variables from X(hoof_set_66): force_area, max_force, mean_force, force_variance, force_skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8716a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five_vars:  1320 True\n"
     ]
    }
   ],
   "source": [
    "## to extract those five variables to the list four_vars with length (264*5) \n",
    "\n",
    "# func to achieve the task in this block \n",
    "def extract_5var(hoof_set_66):\n",
    "    # import skewness measure \n",
    "    from scipy.stats import skew\n",
    "    # init vars\n",
    "    vars = []\n",
    "    \n",
    "    # loop thourgh all matrices and create four_vars \n",
    "    for i in range(hoof_set_66.shape[0]):\n",
    "        # take one matrix and try \n",
    "        one_mat = np.array(hoof_set_66[i], dtype = int)\n",
    "        #print(one_mat) \n",
    "\n",
    "        # obtain all non-zero elements\n",
    "        arr_no_0 = one_mat[np.where(one_mat!=0)]\n",
    "        #print(arr_no_0.shape)\n",
    "\n",
    "        # force_area is length \n",
    "        force_area = len(arr_no_0)\n",
    "        # max force is the max in the list\n",
    "        max_force = max(arr_no_0)\n",
    "        # mean force is the mean value\n",
    "        mean_force = np.mean(arr_no_0)\n",
    "        # obtain variance of this list \n",
    "        force_variance = np.var(arr_no_0)\n",
    "        # force skew\n",
    "        force_skew = skew(sorted(arr_no_0))\n",
    "\n",
    "        # check \n",
    "        #print(\"force_area, max_force, mean_force, force_variance: \", force_area, max_force, mean_force, force_variance)\n",
    "\n",
    "        # append \n",
    "        vars.append(force_area)\n",
    "        vars.append(max_force)\n",
    "        vars.append(mean_force)\n",
    "        vars.append(force_variance)\n",
    "        vars.append(force_skew)\n",
    "    \n",
    "    return vars\n",
    "\n",
    "# use this funciton and check len\n",
    "five_vars = extract_5var(hoof_set_66)\n",
    "print(\"five_vars: \", len(five_vars), len(five_vars)==264*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0d1ce",
   "metadata": {},
   "source": [
    "### (2023-03-03) Want to sum these six variables for one cow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f52aae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "five_vars:  1320\n",
      "sum_six:  330 True\n"
     ]
    }
   ],
   "source": [
    "print(\"five_vars: \", len(five_vars))\n",
    "\n",
    "# every 20 number is from one cow [(var1, var2, var3, var4, var5)*4]\n",
    "\n",
    "def sum_six_var(five_vars):\n",
    "    # init sum_six as the result \n",
    "    sum_six = [] \n",
    "    # loop through all element with a step of 20 \n",
    "    for i in range(0, len(five_vars), 20):\n",
    "        # 20 number as a cow \n",
    "        a_cow = five_vars[i:i+20]\n",
    "        # init for a cow\n",
    "        #var_cow = [] \n",
    "        #print(a_cow)\n",
    "        # sum each var \n",
    "        var1 = sum([a_cow[0], a_cow[5], a_cow[10], a_cow[15]])\n",
    "        var2 = sum([a_cow[0+1], a_cow[5+1], a_cow[10+1], a_cow[15+1]])\n",
    "        var3 = sum([a_cow[0+2], a_cow[5+2], a_cow[10+2], a_cow[15+2]])\n",
    "        var4 = sum([a_cow[0+3], a_cow[5+3], a_cow[10+3], a_cow[15+3]])\n",
    "        var5 = sum([a_cow[0+4], a_cow[5+4], a_cow[10+4], a_cow[15+4]])\n",
    "        # append var for a cow \n",
    "        sum_six += [var1, var2, var3, var4, var5]\n",
    "        \n",
    "    return sum_six\n",
    "\n",
    "sum_six = sum_six_var(five_vars)\n",
    "print(\"sum_six: \", len(sum_six), len(sum_six) == len(five_vars)/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce9dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check length:  66\n"
     ]
    }
   ],
   "source": [
    "six_vars = np.array(sum_six).reshape((66, 5))\n",
    "\n",
    "# create binary label \n",
    "\"\"\"\n",
    "arg: \n",
    "    -- list with 1, 2, 3 \n",
    "return: \n",
    "    -- list with 0, 1\n",
    "\"\"\"\n",
    "def binarize_ls_hoof(ls): \n",
    "    # init new_ls\n",
    "    new_ls = []\n",
    "    for i in ls: \n",
    "        if i > 2.5: \n",
    "            new_ls.append(1) \n",
    "        else: \n",
    "            new_ls.append(0) \n",
    "\n",
    "    print(\"check length: \", len(new_ls))\n",
    "    return new_ls\n",
    "\n",
    "Bi_Y = binarize_ls_hoof(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "944ad5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix: \n",
      " [[17  0]\n",
      " [ 3  0]]\n",
      "Accuracy :\n",
      "  85.0\n",
      "F1 :\n",
      "  0.45945945945945943\n",
      "[[-4.94909248e-03 -8.00505237e-04  1.01442099e-02 -1.59105570e-06\n",
      "  -2.37912217e-04]] [-3.5912986e-06]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# logistic review: https://www.geeksforgeeks.org/ml-logistic-regression-using-python/\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def training(X, Y, test_size, model):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        # our X\n",
    "        X, \n",
    "        # our Y\n",
    "        Y, \n",
    "        # training / testing ratio\n",
    "        test_size=0.3, \n",
    "        random_state=12345)\n",
    "\n",
    "    print(\"y_test: \", y_test)\n",
    "\n",
    "    classifier = model\n",
    "    clf = classifier.fit(x_train, y_train)\n",
    "    \n",
    "    # return model for parameter \n",
    "    return classifier, y_test, x_test\n",
    "\n",
    "# using training function \n",
    "classifier, y_test, x_test = training(X=six_vars, \n",
    "                      Y=np.array(Bi_Y), \n",
    "                      test_size=0.3, \n",
    "                      model=LogisticRegression(random_state=0))\n",
    "# testing \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "# Function to calculate accuracy\n",
    "def cal_accuracy(y_test, y_pred):\n",
    "      \n",
    "    print(\"Confusion Matrix: \\n\",\n",
    "        confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "    print (\"Accuracy :\\n \",\n",
    "    accuracy_score(y_test,y_pred)*100)\n",
    "      \n",
    "    print(\"F1 :\\n \",\n",
    "    f1_score(y_test, y_pred, average='macro'))\n",
    "    \n",
    "# get prediction    \n",
    "y_pred = classifier.predict(x_test)\n",
    "# print accuracy\n",
    "cal_accuracy(y_test, y_pred) \n",
    "# try obtain coefficient\n",
    "\"\"\"one of the benefits from logistic regression\"\"\"\n",
    "print(classifier.coef_, classifier.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c8e881",
   "metadata": {},
   "source": [
    "### Now we need to create two datasets: \n",
    "### one is original_vars (66, 20), \n",
    "### another use asymm_vars (66, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "576c4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this block is to create original_vars (20, 66) \n",
    "\n",
    "# numpy reshape will do this\n",
    "original_vars = np.array(five_vars).reshape((66, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd835be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this block is to create asymm_vars (66, 20) \n",
    "\n",
    "# for each cow each variable\n",
    "# the asymmetric index is  abs(0.5 * ((X_hoof - X_hoof_ave)/(X_hoof + X_hoof_ave)))\n",
    "# the data used is five_vars \n",
    "\n",
    "# helper func \n",
    "\"\"\"\n",
    "args: \n",
    "    --a set of variable in one cow (four hoofs as a list)\n",
    "return: \n",
    "    --a list of asymm index: abs(0.5 * ((X_hoof - X_hoof_ave)/(X_hoof + X_hoof_ave)))\n",
    "\"\"\"\n",
    "def asymm(ls_4):\n",
    "    # compute average value\n",
    "    x_ave = sum(ls_4)/4\n",
    "    # init result list \n",
    "    result = [] \n",
    "    # loop thourgh the input list\n",
    "    for i in ls_4: \n",
    "        # compute asymm index \n",
    "        asy_idx = abs(0.5*((i-x_ave)/(i+x_ave))) * 100\n",
    "        # append to result\n",
    "        result.append(asy_idx)\n",
    "    return result\n",
    "\n",
    "# func to do this job \n",
    "\"\"\"\n",
    "args: \n",
    "    --five_vars (a list)\n",
    "return: \n",
    "    --asymm_vars numpy array with shape (20, 66) \n",
    "\"\"\"\n",
    "def create_asymm(five_vars): \n",
    "\n",
    "    # init a result ls\n",
    "    result = []\n",
    "    # take every 20 elements as a cow \n",
    "    for i in range(0, len(five_vars), 20):\n",
    "        cow = five_vars[i: i+20]\n",
    "        #print(len(cow))\n",
    "        # now we a cow as a list of 20 \n",
    "        # use asymm(ls_4) func\n",
    "        # we have five variables \n",
    "        for i in range(5):\n",
    "            one_hoof = asymm([cow[i], cow[i+5], cow[i+10], cow[i+15]])\n",
    "            # append each value to result \n",
    "            for i in one_hoof: \n",
    "                result.append(i)\n",
    "            \n",
    "    return result\n",
    "\n",
    "# create asymm as a list by the function \n",
    "asymm_vars = create_asymm(five_vars)\n",
    "# reshape the asymm as an array \n",
    "asymm_vars = np.array(asymm_vars).reshape((66, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d686c04",
   "metadata": {},
   "source": [
    "# Correlate variables with NRS score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe8aa231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have inputs: \n",
      "original_vars:  (66, 20)\n",
      "asymm_vars:  (66, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Have inputs: \")\n",
    "print(\"original_vars: \", original_vars.shape)\n",
    "print(\"asymm_vars: \", asymm_vars.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b3950",
   "metadata": {},
   "source": [
    "### Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194dd312",
   "metadata": {},
   "source": [
    "### X: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d58a1933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "shape:  (66, 20)\n",
      "min():  0.7355341457608107\n",
      "max():  216372.1292925749\n",
      "mean():  1728.4386594405726\n",
      "std():  7704.9424426548885\n",
      "-------------------------\n",
      "-------------------------\n",
      "shape:  (66, 20)\n",
      "min():  -0.22423309948821601\n",
      "max():  27.85792265557465\n",
      "mean():  0.0\n",
      "std():  0.9999999999999998\n",
      "-------------------------\n",
      "-------------------------\n",
      "shape:  (66, 20)\n",
      "min():  0.029994001199760045\n",
      "max():  44.82039286586423\n",
      "mean():  9.169874818188665\n",
      "std():  8.024776604846215\n",
      "-------------------------\n",
      "-------------------------\n",
      "shape:  (66, 20)\n",
      "min():  -1.1389576641260357\n",
      "max():  4.442555824687504\n",
      "mean():  -7.266914343001025e-17\n",
      "std():  0.9999999999999999\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# function to inspect dataset \n",
    "def inspect_stat(X):\n",
    "    print(\"-------------------------\")\n",
    "    print(\"shape: \", X.shape)\n",
    "    print(\"min(): \", X.min())\n",
    "    print(\"max(): \", X.max())\n",
    "    print(\"mean(): \", X.mean())\n",
    "    print(\"std(): \", X.std())\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "# standize X \n",
    "def standard(X):\n",
    "    temp = X - X.mean()\n",
    "    X_nor = temp /  X.std()\n",
    "    return X_nor\n",
    "\n",
    "inspect_stat(original_vars)\n",
    "original_vars_nor = standard(original_vars) \n",
    "inspect_stat(original_vars_nor)\n",
    "\n",
    "inspect_stat(asymm_vars)\n",
    "asymm_vars_nor = standard(asymm_vars) \n",
    "inspect_stat(asymm_vars_nor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5164aad1",
   "metadata": {},
   "source": [
    "### Y: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79443ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check length:  66\n",
      "----- Set of value in the list -----\n",
      "[0, 1]\n",
      "----- Count of value in the list -----\n",
      "[58, 8]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create binary label \n",
    "\"\"\"\n",
    "arg: \n",
    "    -- list with 1, 2, 3 \n",
    "return: \n",
    "    -- list with 0, 1\n",
    "\"\"\"\n",
    "def binarize_ls_hoof(ls): \n",
    "    # init new_ls\n",
    "    new_ls = []\n",
    "    for i in ls: \n",
    "        if i > 2.5: \n",
    "            new_ls.append(1) \n",
    "        else: \n",
    "            new_ls.append(0) \n",
    "\n",
    "    print(\"check length: \", len(new_ls))\n",
    "    return new_ls\n",
    "\n",
    "Bi_Y = binarize_ls_hoof(Y)\n",
    "inspect_y(Bi_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ebd1ec",
   "metadata": {},
   "source": [
    "### Spliting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "150f83a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#####################################################################\n",
    "############## spliting into train and test and reshape for tf  #####\n",
    "#####################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    # our X\n",
    "    asymm_vars, \n",
    "    # our Y\n",
    "    np.array(Bi_Y), \n",
    "    # training / testing ratio\n",
    "    test_size=0.3, \n",
    "    random_state=12345)\n",
    "\n",
    "print(\"y_test: \", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd3a227",
   "metadata": {},
   "source": [
    "# NOW, let test on different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9944d82",
   "metadata": {},
   "source": [
    "### For example, logistic regression, KNN, SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f93fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# logistic review: https://www.geeksforgeeks.org/ml-logistic-regression-using-python/\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "625d3615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have inputs: \n",
      "original_vars_nor:  (66, 20)\n",
      "asymm_vars:  (66, 20)\n",
      "Bi_Y:  66\n"
     ]
    }
   ],
   "source": [
    "print(\"Have inputs: \")\n",
    "print(\"original_vars_nor: \", original_vars_nor.shape)\n",
    "print(\"asymm_vars: \", asymm_vars.shape)\n",
    "print(\"Bi_Y: \", len(Bi_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c618e53",
   "metadata": {},
   "source": [
    "### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d92006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Function to transform y_pred to binary \n",
    "def prob_to_pred(y_pred): \n",
    "    result = [] \n",
    "    for i in y_pred: \n",
    "        if i[0] >= 0.5: \n",
    "            result.append(1) \n",
    "        else: \n",
    "            result.append(0) \n",
    "    return np.array(result)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def cal_accuracy(y_test, y_pred):\n",
    "      \n",
    "    print(\"Confusion Matrix: \\n\",\n",
    "        confusion_matrix(y_test, y_pred))\n",
    "      \n",
    "    print (\"Accuracy :\\n \",\n",
    "    accuracy_score(y_test,y_pred)*100)\n",
    "      \n",
    "    print(\"F1 :\\n \",\n",
    "    f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df36c3",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "409e471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, Y, test_size, model):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        # our X\n",
    "        X, \n",
    "        # our Y\n",
    "        Y, \n",
    "        # training / testing ratio\n",
    "        test_size=0.3, \n",
    "        random_state=12345)\n",
    "\n",
    "    print(\"y_test: \", y_test)\n",
    "\n",
    "    classifier = model\n",
    "    clf = classifier.fit(x_train, y_train)\n",
    "    \n",
    "    # return model for parameter \n",
    "    return classifier, y_test, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2722b77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----2024-06-19: significance test: \n"
     ]
    }
   ],
   "source": [
    "print(\"-----2024-06-19: significance test: \")\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Assume 'X' is your independent variables and 'y' is your dependent variable\n",
    "# X should be a DataFrame of your predictors, and y should be a Series of your target\n",
    "X=asymm_vars\n",
    "y=np.array(Bi_Y)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "shrink number of variables from 20 to 5\n",
    "args: \n",
    "    list with length 20 \n",
    "return: \n",
    "    list with length 5\n",
    "\"\"\"\n",
    "def sum_each_var(original_list):\n",
    "    indices_group1 = [0, 5, 10, 15]\n",
    "    indices_group2 = [i+1 for i in indices_group1]\n",
    "    indices_group3 = [i+1 for i in indices_group2]\n",
    "    indices_group4 = [i+1 for i in indices_group3]\n",
    "    indices_group5 = [i+1 for i in indices_group4]\n",
    "    all_indices = [indices_group1, indices_group2, indices_group3, indices_group4, indices_group5]\n",
    "\n",
    "    # Sum elements from each group\n",
    "    result = [sum(original_list[i] for i in indices_group) for indices_group in all_indices]\n",
    "    #print(result)\n",
    "    return result\n",
    "    \n",
    "\"\"\"using func for each row of X\"\"\"\n",
    "X_small = np.zeros((66, 5))\n",
    "for r in range(len(X)): \n",
    "    X_small[r] = sum_each_var(X[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97aca36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----2024-06-19: significance test: \n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.328409\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   66\n",
      "Model:                          Logit   Df Residuals:                       60\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Wed, 19 Jun 2024   Pseudo R-squ.:                  0.1108\n",
      "Time:                        16:36:51   Log-Likelihood:                -21.675\n",
      "converged:                       True   LL-Null:                       -24.376\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.3688\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.6634      1.653     -1.611      0.107      -5.903       0.577\n",
      "x1             0.0097      0.038      0.254      0.799      -0.065       0.084\n",
      "x2            -0.0169      0.069     -0.245      0.806      -0.152       0.118\n",
      "x3             0.0804      0.042      1.911      0.056      -0.002       0.163\n",
      "x4            -0.0501      0.040     -1.260      0.207      -0.128       0.028\n",
      "x5            -0.0341      0.037     -0.929      0.353      -0.106       0.038\n",
      "==============================================================================\n",
      "T-values: [-1.61120192  0.25445    -0.24542695  1.9109981  -1.26047978 -0.92911318]\n",
      "P-values: [0.10713572 0.79914792 0.80612586 0.05600482 0.20749634 0.35283044]\n"
     ]
    }
   ],
   "source": [
    "print(\"-----2024-06-19: significance test: \")\n",
    "\n",
    "X = X_small\n",
    "# Add intercept to X\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit logistic regression model\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "# Print summary to see coefficients and their standard errors\n",
    "print(result.summary())\n",
    "\n",
    "# Access coefficients and standard errors\n",
    "coefficients = result.params\n",
    "std_errors = result.bse\n",
    "\n",
    "# Calculate t-values\n",
    "t_values = coefficients / std_errors\n",
    "\n",
    "# Calculate p-values (two-tailed)\n",
    "p_values = result.pvalues\n",
    "\n",
    "# Print t-values and p-values\n",
    "print(\"T-values:\", t_values)\n",
    "print(\"P-values:\", p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e7c1bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix: \n",
      " [[15  2]\n",
      " [ 3  0]]\n",
      "Accuracy :\n",
      "  75.0\n",
      "F1 :\n",
      "  0.42857142857142855\n",
      "[[ 0.04956705 -0.04866325  0.13328815 -0.06322898 -0.09882746]] [-3.78530492]\n"
     ]
    }
   ],
   "source": [
    "# using training function \n",
    "classifier, y_test, x_test = training(X= X_small, \n",
    "                      Y=np.array(Bi_Y), \n",
    "                      test_size=0.3, \n",
    "                      model=LogisticRegression(random_state=0))\n",
    "# get prediction    \n",
    "y_pred = classifier.predict(x_test)\n",
    "# print accuracy\n",
    "cal_accuracy(y_test, y_pred) \n",
    "# try obtain coefficient\n",
    "\"\"\"one of the benefits from logistic regression\"\"\"\n",
    "print(classifier.coef_, classifier.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27bc60",
   "metadata": {},
   "source": [
    "### Dataset original_vars_nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebb22968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix: \n",
      " [[17  0]\n",
      " [ 3  0]]\n",
      "Accuracy :\n",
      "  85.0\n",
      "F1 :\n",
      "  0.45945945945945943\n",
      "[[ 1.72986669e-02  4.15135779e-02  2.34718502e-03  5.84149062e-01\n",
      "   5.54447814e-05 -9.51996089e-03 -7.90644471e-03 -1.00209616e-03\n",
      "  -6.17656880e-01  4.82223630e-07 -4.70328287e-03 -9.10442263e-03\n",
      "   4.40236074e-03 -6.98142978e-02 -7.74538372e-05 -6.80732114e-03\n",
      "  -3.95110142e-02 -3.02888179e-03 -6.03142322e-01 -1.36838584e-04]] [-2.34061951]\n"
     ]
    }
   ],
   "source": [
    "# using training function \n",
    "classifier, y_test, x_test = training(X=original_vars_nor, \n",
    "                      Y=np.array(Bi_Y), \n",
    "                      test_size=0.3, \n",
    "                      model=LogisticRegression(random_state=0))\n",
    "# get prediction    \n",
    "y_pred = classifier.predict(x_test)\n",
    "# print accuracy\n",
    "cal_accuracy(y_test, y_pred) \n",
    "# try obtain coefficient\n",
    "\"\"\"one of the benefits from logistic regression\"\"\"\n",
    "print(classifier.coef_, classifier.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ac0fe",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "406b3ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== md:  4 ===== ml:  2 =====\n",
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix: \n",
      " [[15  2]\n",
      " [ 3  0]]\n",
      "Accuracy :\n",
      "  75.0\n",
      "F1 :\n",
      "  0.42857142857142855\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "max_depth = [4]   \n",
    "min_samples_leaf = [2]\n",
    "\n",
    "# search through md and ml \n",
    "for md in max_depth: \n",
    "    for ml in min_samples_leaf: \n",
    "        print(\"====== md: \", md, \"===== ml: \", ml, \"=====\")\n",
    "        # using training function \n",
    "        classifier, y_test, x_test = training(X=X_small, \n",
    "                              Y=np.array(Bi_Y), \n",
    "                              test_size=0.3, \n",
    "                              model=DecisionTreeClassifier(random_state = 0, max_depth=md, min_samples_leaf=ml))\n",
    "        # get prediction    \n",
    "        y_pred = classifier.predict(x_test)\n",
    "        # print accuracy\n",
    "        cal_accuracy(y_test, y_pred) \n",
    "        print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5830aeb4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== md:  4 ===== ml:  2 =====\n",
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix: \n",
      " [[17  0]\n",
      " [ 3  0]]\n",
      "Accuracy :\n",
      "  85.0\n",
      "F1 :\n",
      "  0.45945945945945943\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "max_depth = [4]   \n",
    "min_samples_leaf = [2]\n",
    "\n",
    "# search through md and ml \n",
    "for md in max_depth: \n",
    "    for ml in min_samples_leaf: \n",
    "        print(\"====== md: \", md, \"===== ml: \", ml, \"=====\")\n",
    "        # using training function \n",
    "        classifier, y_test, x_test = training(X=asymm_vars, \n",
    "                              Y=np.array(Bi_Y), \n",
    "                              test_size=0.3, \n",
    "                              model=DecisionTreeClassifier(random_state = 0, max_depth=md, min_samples_leaf=ml))\n",
    "        # get prediction    \n",
    "        y_pred = classifier.predict(x_test)\n",
    "        # print accuracy\n",
    "        cal_accuracy(y_test, y_pred) \n",
    "        print(\"=====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f8e5b",
   "metadata": {},
   "source": [
    "### Use original_vars_nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cab22b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== md:  4 ===== ml:  2 =====\n",
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix: \n",
      " [[17  0]\n",
      " [ 3  0]]\n",
      "Accuracy :\n",
      "  85.0\n",
      "F1 :\n",
      "  0.45945945945945943\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "max_depth = [4]   \n",
    "min_samples_leaf = [2]\n",
    "\n",
    "# search through md and ml \n",
    "for md in max_depth: \n",
    "    for ml in min_samples_leaf: \n",
    "        print(\"====== md: \", md, \"===== ml: \", ml, \"=====\")\n",
    "        # using training function \n",
    "        classifier, y_test, x_test = training(X=original_vars_nor, \n",
    "                              Y=np.array(Bi_Y), \n",
    "                              test_size=0.3, \n",
    "                              model=DecisionTreeClassifier(random_state = 0, max_depth=md, min_samples_leaf=ml))\n",
    "        # get prediction    \n",
    "        y_pred = classifier.predict(x_test)\n",
    "        # print accuracy\n",
    "        cal_accuracy(y_test, y_pred) \n",
    "        print(\"=====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c3c3f",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a551a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from : https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-python/\n",
    "# https://stackoverflow.com/questions/65898399/no-module-named-sklearn-datasets-samples-generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39698c2a",
   "metadata": {},
   "source": [
    "### SVM with weighted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23dfec44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\15680\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp39-cp39-win_amd64.whl (8.4 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\15680\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\15680\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\15680\\anaconda3\\lib\\site-packages (from scikit-learn) (1.7.3)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.1\n"
     ]
    }
   ],
   "source": [
    "# update SVM \n",
    "# https://stackoverflow.com/questions/72246343/importerror-cannot-import-name-decisionboundarydisplay-from-sklearn-inspecti\n",
    "#!pip install -U scikit-learn --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd653d98",
   "metadata": {},
   "source": [
    "# Try SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6adb41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix: \n",
      " [[15  2]\n",
      " [ 3  0]]\n",
      "Accuracy :\n",
      "  75.0\n",
      "F1 :\n",
      "  0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "C=[100]\n",
    "class_weight=[{1:1}]\n",
    "\n",
    "for c in C: \n",
    "    for cw in class_weight: \n",
    "        # using training function \n",
    "        classifier, y_test, x_test = training(X=X_small, \n",
    "                              Y=np.array(Bi_Y), \n",
    "                              test_size=0.3, \n",
    "                              model=svm.SVC(kernel=\"rbf\", C=c, class_weight=cw))\n",
    "        # get prediction    \n",
    "        y_pred = classifier.predict(x_test)\n",
    "        # print accuracy\n",
    "        cal_accuracy(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7607b1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix: \n",
      " [[17  0]\n",
      " [ 3  0]]\n",
      "Accuracy :\n",
      "  85.0\n",
      "F1 :\n",
      "  0.45945945945945943\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "C=[100]\n",
    "class_weight=[{1:1}]\n",
    "\n",
    "for c in C: \n",
    "    for cw in class_weight: \n",
    "        # using training function \n",
    "        classifier, y_test, x_test = training(X=original_vars_nor, \n",
    "                              Y=np.array(Bi_Y), \n",
    "                              test_size=0.3, \n",
    "                              model=svm.SVC(kernel=\"rbf\", C=c, class_weight=cw))\n",
    "        # get prediction    \n",
    "        y_pred = classifier.predict(x_test)\n",
    "        # print accuracy\n",
    "        cal_accuracy(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2353427b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0]\n",
      "Confusion Matrix: \n",
      " [[16  1]\n",
      " [ 2  1]]\n",
      "Accuracy :\n",
      "  85.0\n",
      "F1 :\n",
      "  0.6571428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "C=[100]\n",
    "class_weight=[{1:1}]\n",
    "\n",
    "for c in C: \n",
    "    for cw in class_weight: \n",
    "        # using training function \n",
    "        classifier, y_test, x_test = training(X=asymm_vars, \n",
    "                              Y=np.array(Bi_Y), \n",
    "                              test_size=0.3, \n",
    "                              model=svm.SVC(kernel=\"rbf\", C=c, class_weight=cw))\n",
    "        # get prediction    \n",
    "        y_pred = classifier.predict(x_test)\n",
    "        # print accuracy\n",
    "        cal_accuracy(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae8e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
